{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9905715,
          "sourceType": "datasetVersion",
          "datasetId": 6085716
        }
      ],
      "dockerImageVersionId": 30788,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "\n",
        "sandeepchatterjee66_ml4crypto_path = kagglehub.dataset_download('sandeepchatterjee66/ml4crypto')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "mAD6-JreebmC"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "WIzWE2dTebmD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T13:16:05.539325Z",
          "iopub.execute_input": "2024-11-14T13:16:05.539724Z",
          "iopub.status.idle": "2024-11-14T13:16:05.929902Z",
          "shell.execute_reply.started": "2024-11-14T13:16:05.539675Z",
          "shell.execute_reply": "2024-11-14T13:16:05.928875Z"
        },
        "id": "jEMKi2-1ebmD"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/TrainingData.csv\")\n",
        "data\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T13:16:05.931416Z",
          "iopub.execute_input": "2024-11-14T13:16:05.931811Z",
          "iopub.status.idle": "2024-11-14T13:16:06.01869Z",
          "shell.execute_reply.started": "2024-11-14T13:16:05.931777Z",
          "shell.execute_reply": "2024-11-14T13:16:06.017686Z"
        },
        "id": "u507JvXRebmE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "1d2b7984-1b2c-4f4f-f7e8-517e8385522e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       BID                                          Bitstream  class\n",
              "0        0  1000111010111101101100110111001111001000101111...      1\n",
              "1        1  1101111100101011111111101101010001110110000010...      1\n",
              "2        2  0011001010001010100100011101000111110100101111...      0\n",
              "3        3  1101010110000110100001001100111101000000110001...      1\n",
              "4        4  1010111100001001000101010010111010011101001100...      1\n",
              "...    ...                                                ...    ...\n",
              "1995  1995  1110110011110100001111101111010110011000001110...      0\n",
              "1996  1996  0100010100011110101110000110100101100000011001...      1\n",
              "1997  1997  1100001010100011010001110001010010101010101100...      0\n",
              "1998  1998  0011110000001110101101111110110100010010100011...      1\n",
              "1999  1999  0100000010100101000000011010011011011111011011...      1\n",
              "\n",
              "[2000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66a2516f-e145-4c37-bf4a-5f5a579de121\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BID</th>\n",
              "      <th>Bitstream</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1000111010111101101100110111001111001000101111...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1101111100101011111111101101010001110110000010...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0011001010001010100100011101000111110100101111...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1101010110000110100001001100111101000000110001...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1010111100001001000101010010111010011101001100...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "      <td>1110110011110100001111101111010110011000001110...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "      <td>0100010100011110101110000110100101100000011001...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "      <td>1100001010100011010001110001010010101010101100...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "      <td>0011110000001110101101111110110100010010100011...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "      <td>0100000010100101000000011010011011011111011011...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a2516f-e145-4c37-bf4a-5f5a579de121')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66a2516f-e145-4c37-bf4a-5f5a579de121 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66a2516f-e145-4c37-bf4a-5f5a579de121');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e47cd952-8569-46ef-ab42-e810576123d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e47cd952-8569-46ef-ab42-e810576123d1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e47cd952-8569-46ef-ab42-e810576123d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_004e14b8-9eb5-4b1f-bd0f-0dd6cede5733\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_004e14b8-9eb5-4b1f-bd0f-0dd6cede5733 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 2000,\n  \"fields\": [\n    {\n      \"column\": \"BID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 577,\n        \"min\": 0,\n        \"max\": 1999,\n        \"num_unique_values\": 2000,\n        \"samples\": [\n          1860,\n          353,\n          1333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Bitstream\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2000,\n        \"samples\": [\n          \"0000001110101100001101110101001101011111100101101110000011010001000010101001000100101010111000000100010000101100111000000110011001000010001011100000100101101000001000101000110010001000111100000000000001100100111100101110111000111010001001001011001011010110011001000100000010010101010110111110010000111000010100101011001001001011001110010101110001000001000000111010001011000011100001001101010011100011010110110100100111101100001010100000010110111010001011111011100110010101111000001001001111100101111001010100100011101001010101010100000110101100101111000111111010110001011011001010000110110111000110111000010011010100111010110000011100000010001000000011011011011111111111010110110011011111000001111101001101011010100110100110100001001010011100110110001111011000010110000001111011010010100101100110011111011110111001110101100101011110010000000000000000000110110100011011011111111101110110010001110110110111100010111100111001000111101100110001110111001010111110110000101110010011100001100111100010101100001010111110100000000111\",\n          \"1001101010000011110110101101010010011000000111110110110111110111010111111011000101110000011111111011011010110101000101110000000000010100001100111101000111110001011111111101100011101111110111100100010011011011011010110010000011101010111100101001001000011101011000011001001111000100011001001100011111100111111001101001010010011101100001100001110000000000100101111010110001100101000010111111111000001000000001101111100001110010010111101001110100111111111100100010111111000001001100011001101000011001100100010001011110101100101101000100001101110100011100100111001110100110100100000101100001100001111011000111101010101010100000011111110111100001100011000111111011010010011100000000101101101000110001000001111001100100010011010111001111101001010001000110101010110011010010111111111000011110010011011111110101010100011001010111001101111101111010111000100011011010000001000001001101000101111111111101110110011110101011010001111000100011100000011110011010011011110011111001001111001111010100111101001111111100101000110000100101011011\",\n          \"1111111101111001011111101010100110001101001010010110111110001010100111101100101101111101110011110010011110100101010011001001010110111001101100100101011100000010101110001011000111111000101101101000000110101001101011101000011011010000011110010000010000111011001010011110101001011011011001110011001001010110110011111010011101010111010010110111010101000100000101000000000110100000011111110001111010001000101000100110010011110011110110101001100111111111001010010101011101111110000010110010010000100110101010011011000010010010000000101100110010010110000000001101101001010111110000001110010011101000010111000000100100110000110001111111010001011101100010011011010010001110110011011101000000100011100101101111100011100001100101011000011011000110011001110000010100011010101000111010000101101100000000101011011111110110100000110010101101000100111111001111001101001001001111010100000001001101101001011100010000000011010111111100001101011111000111011100100010010011101100111100111111011101011111001010001000111101001000011110010010110011\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "len(data[\"Bitstream\"][0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T13:16:06.020354Z",
          "iopub.execute_input": "2024-11-14T13:16:06.02069Z",
          "iopub.status.idle": "2024-11-14T13:16:06.028359Z",
          "shell.execute_reply.started": "2024-11-14T13:16:06.020649Z",
          "shell.execute_reply": "2024-11-14T13:16:06.027365Z"
        },
        "id": "iamMT3VpebmF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10810aba-aa99-4809-b321-6c108bd1b6e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import numpy as np\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "# import os\n",
        "\n",
        "# # Check if TPU is available\n",
        "# import os\n",
        "# import transformers\n",
        "\n",
        "# if 'COLAB_TPU_ADDR' in os.environ:\n",
        "#     TPU = True\n",
        "#     resolver = transformers.TPUMembershipFilter()\n",
        "#     transformers.utils.set_seed(42)\n",
        "# else:\n",
        "#     TPU = False\n",
        "\n",
        "# # Load and preprocess the data\n",
        "# class BitstreamDataset(Dataset):\n",
        "#     def __init__(self, data, tokenizer, max_length=1024):\n",
        "#         self.data = data\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_length = max_length\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         bitstream = self.data['Bitstream'][idx]\n",
        "#         label = self.data['class'][idx]\n",
        "#         inputs = self.tokenizer.encode_plus(bitstream,\n",
        "#                                            max_length=self.max_length,\n",
        "#                                            pad_to_max_length=True,\n",
        "#                                            return_tensors='pt')\n",
        "#         return inputs, label\n",
        "\n",
        "# # Fine-tune GPT on TPU\n",
        "# if TPU:\n",
        "#     import torch_xla\n",
        "#     import torch_xla.core.xla_model as xm\n",
        "#     import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "#     model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "#     dataset = BitstreamDataset(data, tokenizer)\n",
        "#     train_loader = pl.ParallelLoader(dataset, [xm.xla_device()])\n",
        "\n",
        "#     optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "#     scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "#                                                 num_warmup_steps=100,\n",
        "#                                                 num_training_steps=len(train_loader) * 3)\n",
        "\n",
        "#     model.train()\n",
        "#     for epoch in range(3):\n",
        "#         for inputs, labels in train_loader.per_device_loader(xm.xla_device()):\n",
        "#             outputs = model(inputs, labels=labels)\n",
        "#             loss = outputs.loss\n",
        "#             xm.optimizer_step(optimizer)\n",
        "#             scheduler.step()\n",
        "#             xm.mark_step()\n",
        "\n",
        "#     # Evaluate on test set\n",
        "#     model.eval()\n",
        "#     accuracy = 0\n",
        "#     for inputs, labels in test_dataloader:\n",
        "#         outputs = model(inputs)\n",
        "#         predictions = outputs.logits.argmax(dim=1)\n",
        "#         accuracy += (predictions == labels).float().mean()\n",
        "#     print(f'Test accuracy: {accuracy / len(test_dataloader)}')\n",
        "# else:\n",
        "#     # Use CPU/GPU if TPU is not available\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     # Rest of the code remains the same as before"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T13:16:06.029855Z",
          "iopub.execute_input": "2024-11-14T13:16:06.030181Z",
          "iopub.status.idle": "2024-11-14T13:16:06.037136Z",
          "shell.execute_reply.started": "2024-11-14T13:16:06.030143Z",
          "shell.execute_reply": "2024-11-14T13:16:06.036234Z"
        },
        "id": "CGqZr6dpebmF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T13:16:17.319367Z",
          "iopub.status.idle": "2024-11-14T13:16:17.319943Z",
          "shell.execute_reply.started": "2024-11-14T13:16:17.319655Z",
          "shell.execute_reply": "2024-11-14T13:16:17.319685Z"
        },
        "id": "bD8ZynQuebmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055fc305-a1d7-492f-b08b-fbcf62daae8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "pip install peft"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T13:43:07.726081Z",
          "iopub.execute_input": "2024-11-14T13:43:07.727059Z",
          "iopub.status.idle": "2024-11-14T13:43:19.93709Z",
          "shell.execute_reply.started": "2024-11-14T13:43:07.727016Z",
          "shell.execute_reply": "2024-11-14T13:43:19.935868Z"
        },
        "id": "4E1upkfNebmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e9723a-97ae-40da-8147-825696588566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Reload the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "\n",
        "# Evaluation loop with reversed logits\n",
        "model.eval()\n",
        "\n",
        "# Assuming input_ids, attention_mask, and labels are already tensors:\n",
        "# Ensure that all tensors are moved to the appropriate device (GPU or CPU)\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# Reverse the logits function\n",
        "def reverse_logits(logits):\n",
        "    return logits * -1  # Reverse logits by multiplying by -1\n",
        "\n",
        "# Test the model without reversing logits\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    # Calculate accuracy without reversing logits\n",
        "    correct_predictions = (predictions == labels).sum().item()\n",
        "    accuracy_without_reverse = correct_predictions / len(labels) * 100\n",
        "\n",
        "print(f\"Accuracy without reversing logits on the entire dataset: {accuracy_without_reverse:.2f}%\")\n",
        "\n",
        "# Test the model with reversed logits\n",
        "with torch.no_grad():\n",
        "    reversed_logits = reverse_logits(logits)\n",
        "    reversed_predictions = torch.argmax(reversed_logits, dim=-1)\n",
        "\n",
        "    # Calculate accuracy with reversed logits\n",
        "    correct_predictions = (reversed_predictions == labels).sum().item()\n",
        "    accuracy_with_reverse = correct_predictions / len(labels) * 100\n",
        "\n",
        "print(f\"Accuracy with reversed logits on the entire dataset: {accuracy_with_reverse:.2f}%\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T13:51:58.579958Z",
          "iopub.execute_input": "2024-11-14T13:51:58.581016Z",
          "iopub.status.idle": "2024-11-14T13:51:58.949788Z",
          "shell.execute_reply.started": "2024-11-14T13:51:58.58096Z",
          "shell.execute_reply": "2024-11-14T13:51:58.948531Z"
        },
        "id": "V3bf7D5XebmH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import random\n",
        "from peft import get_peft_model, LoraConfig, PeftModel\n",
        "\n",
        "df = data\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "\n",
        "# Ensure the model's config uses the pad_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Resize embeddings for new pad token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "\n",
        "# LoRA configuration for efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # rank of low-rank matrices (you can tune this)\n",
        "    lora_alpha=32,  # scaling factor for LoRA layers\n",
        "    target_modules=[\"attn.c_attn\", \"attn.c_proj\"],  # which modules to apply LoRA to\n",
        "    lora_dropout=0.1,  # dropout for LoRA layers\n",
        "    bias=\"none\",  # no bias term in LoRA layers\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.to(device)\n",
        "\n",
        "# Load your dataset (adjust the file path and column names)\n",
        "#df = pd.read_csv('/kaggle/input/ml4crypto/TrainingData.csv')  # Replace with your dataset file path\n",
        "\n",
        "# Extract binary strings and labels\n",
        "binary_strings = df['Bitstream'].tolist()\n",
        "labels = df['class'].tolist()\n",
        "\n",
        "# Preprocess the binary strings by splitting them into halves and XOR'ing the halves\n",
        "def xor_preprocess(binary_string):\n",
        "    # Split the string into two halves\n",
        "    s1 = binary_string[:512]\n",
        "    s2 = binary_string[512:]\n",
        "\n",
        "    # XOR the halves\n",
        "    s1_int = int(s1, 2)\n",
        "    s2_int = int(s2, 2)\n",
        "    xor_result = s1_int ^ s2_int\n",
        "\n",
        "    # Convert XOR result back to binary string (512 bits)\n",
        "    xor_binary_string = format(xor_result, '512b')\n",
        "    return xor_binary_string\n",
        "\n",
        "# Apply preprocessing to all binary strings\n",
        "processed_binary_strings = [xor_preprocess(s) for s in binary_strings]\n",
        "\n",
        "# Tokenize the processed binary strings\n",
        "inputs = tokenizer(processed_binary_strings, padding=True, truncation=True, max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "# Convert to tensors and move to the appropriate device\n",
        "input_ids = inputs['input_ids'].to(device)\n",
        "attention_mask = inputs['attention_mask'].to(device)\n",
        "labels = torch.tensor(labels).to(device)\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 15  # Update epoch count to 10 as per your request\n",
        "best_accuracy = 0.0\n",
        "best_model_path = \"gpt_best_model.pth\"\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        # Move the batch to the device\n",
        "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Save the model if it's the best\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved with accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Reload the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "\n",
        "# Evaluation loop with random sampling and reversed logits\n",
        "model.eval()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T16:09:42.821633Z",
          "iopub.execute_input": "2024-11-14T16:09:42.822221Z",
          "iopub.status.idle": "2024-11-14T16:16:39.781433Z",
          "shell.execute_reply.started": "2024-11-14T16:09:42.822182Z",
          "shell.execute_reply": "2024-11-14T16:16:39.780523Z"
        },
        "id": "BfNybZ-_ebmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f8c3a1-b194-40d6-843a-1fb8564074e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 0.7280282241478563\n",
            "Validation Accuracy: 48.50%\n",
            "Best model saved with accuracy: 48.50%\n",
            "Epoch 2/15, Loss: 0.7053972987830639\n",
            "Validation Accuracy: 53.00%\n",
            "Best model saved with accuracy: 53.00%\n",
            "Epoch 3/15, Loss: 0.706314246468246\n",
            "Validation Accuracy: 47.00%\n",
            "Epoch 4/15, Loss: 0.6977106180042029\n",
            "Validation Accuracy: 49.75%\n",
            "Epoch 5/15, Loss: 0.6942617348954081\n",
            "Validation Accuracy: 49.75%\n",
            "Epoch 6/15, Loss: 0.6937116514518857\n",
            "Validation Accuracy: 49.00%\n",
            "Epoch 7/15, Loss: 0.6957987089455128\n",
            "Validation Accuracy: 49.75%\n",
            "Epoch 8/15, Loss: 0.6938062854111194\n",
            "Validation Accuracy: 51.25%\n",
            "Epoch 9/15, Loss: 0.6907831660285592\n",
            "Validation Accuracy: 49.25%\n",
            "Epoch 10/15, Loss: 0.6924337783828378\n",
            "Validation Accuracy: 49.00%\n",
            "Epoch 11/15, Loss: 0.6779078487679362\n",
            "Validation Accuracy: 49.50%\n",
            "Epoch 12/15, Loss: 0.684359211884439\n",
            "Validation Accuracy: 47.50%\n",
            "Epoch 13/15, Loss: 0.6861557794362306\n",
            "Validation Accuracy: 48.50%\n",
            "Epoch 14/15, Loss: 0.679899048730731\n",
            "Validation Accuracy: 46.25%\n",
            "Epoch 15/15, Loss: 0.6790768676623702\n",
            "Validation Accuracy: 46.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-7b21c4af369d>:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): GPT2ForSequenceClassification(\n",
              "      (transformer): GPT2Model(\n",
              "        (wte): Embedding(50257, 768)\n",
              "        (wpe): Embedding(1024, 768)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "        (h): ModuleList(\n",
              "          (0-11): 12 x GPT2Block(\n",
              "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): GPT2SdpaAttention(\n",
              "              (c_attn): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=2304, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (c_proj): lora.Linear(\n",
              "                (base_layer): Conv1D(nf=768, nx=768)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): GPT2MLP(\n",
              "              (c_fc): Conv1D(nf=3072, nx=768)\n",
              "              (c_proj): Conv1D(nf=768, nx=3072)\n",
              "              (act): NewGELUActivation()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (score): Linear(in_features=768, out_features=2, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the best model on the entire validation dataset\n",
        "model.eval()\n",
        "\n",
        "# Initialize counters for accuracy\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "# Loop over the validation set\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "# Calculate final accuracy\n",
        "accuracy = total_correct / total_samples * 100\n",
        "print(f\"Accuracy on the entire validation dataset: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T16:17:22.501397Z",
          "iopub.execute_input": "2024-11-14T16:17:22.501774Z",
          "iopub.status.idle": "2024-11-14T16:17:26.001202Z",
          "shell.execute_reply.started": "2024-11-14T16:17:22.501738Z",
          "shell.execute_reply": "2024-11-14T16:17:26.0002Z"
        },
        "id": "q9arlmRZebmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c349e733-57d6-4b8b-bc7e-1adb9e313d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the entire validation dataset: 53.00%\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# LoRA configuration for efficient fine-tuning with BERT\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # rank of low-rank matrices\n",
        "    lora_alpha=32,  # scaling factor for LoRA layers\n",
        "    target_modules=[\"attention.self.query\", \"attention.self.key\", \"attention.self.value\", \"attention.output.dense\"],  # BERT compatible modules\n",
        "    lora_dropout=0.1,  # dropout for LoRA layers\n",
        "    bias=\"none\",  # no bias term in LoRA layers\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "try:\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.to(device)\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please check the target modules. Make sure they match the BERT model structure.\")\n",
        "    raise\n",
        "\n",
        "# Load your dataset (adjust the file path and column names)\n",
        "df = data  # Replace with your dataset file path\n",
        "\n",
        "# Extract binary strings and labels\n",
        "binary_strings = df['Bitstream'].tolist()\n",
        "labels = df['class'].tolist()\n",
        "\n",
        "# Preprocess the binary strings by splitting them into halves and XOR'ing the halves\n",
        "def xor_preprocess(binary_string):\n",
        "    s1 = binary_string[:512]\n",
        "    s2 = binary_string[512:]\n",
        "    s1_int = int(s1, 2)\n",
        "    s2_int = int(s2, 2)\n",
        "    xor_result = s1_int ^ s2_int\n",
        "    xor_binary_string = format(xor_result, '0512b')  # Adjusted to ensure 512 bits\n",
        "    return xor_binary_string\n",
        "\n",
        "# Apply preprocessing to all binary strings\n",
        "processed_binary_strings = [xor_preprocess(s) for s in binary_strings]\n",
        "\n",
        "# Tokenize the processed binary strings\n",
        "inputs = tokenizer(processed_binary_strings, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "# Convert to tensors and move to the appropriate device\n",
        "input_ids = inputs['input_ids'].to(device)\n",
        "attention_mask = inputs['attention_mask'].to(device)\n",
        "labels = torch.tensor(labels).to(device)\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=2)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 20  # Update epoch count to 20\n",
        "best_accuracy = 0.0\n",
        "best_model_path = \"best_model.pth\"\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Save the best model based on validation accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved with accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Reload the best model\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "\n",
        "# Final evaluation on the full dataset can now proceed here.\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VsMtHo7GebmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a73f11-b612-4258-bda1-2b7735063587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.7050536767393351\n",
            "Validation Accuracy: 41.50%\n",
            "Best model saved with accuracy: 41.50%\n",
            "Epoch 2/20, Loss: 0.6985265891999006\n",
            "Validation Accuracy: 58.50%\n",
            "Best model saved with accuracy: 58.50%\n",
            "Epoch 3/20, Loss: 0.6949944455549121\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 4/20, Loss: 0.6966877183318139\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 5/20, Loss: 0.6979087771475315\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 6/20, Loss: 0.6975773316621781\n",
            "Validation Accuracy: 58.50%\n",
            "Epoch 7/20, Loss: 0.6938082890212536\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 8/20, Loss: 0.695544774979353\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 9/20, Loss: 0.6936459349095822\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 10/20, Loss: 0.6969192644208669\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 11/20, Loss: 0.6975535332411528\n",
            "Validation Accuracy: 58.50%\n",
            "Epoch 12/20, Loss: 0.6931470593810082\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 13/20, Loss: 0.6929664281755685\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 14/20, Loss: 0.6973433938622474\n",
            "Validation Accuracy: 58.50%\n",
            "Epoch 15/20, Loss: 0.6962995431572199\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 16/20, Loss: 0.6941995688527822\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 17/20, Loss: 0.6922929346561432\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 18/20, Loss: 0.6942176257818937\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 19/20, Loss: 0.6971428709477187\n",
            "Validation Accuracy: 41.50%\n",
            "Epoch 20/20, Loss: 0.6940503816306591\n",
            "Validation Accuracy: 41.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-47456f2e6641>:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): BertForSequenceClassification(\n",
              "      (bert): BertModel(\n",
              "        (embeddings): BertEmbeddings(\n",
              "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "          (position_embeddings): Embedding(512, 768)\n",
              "          (token_type_embeddings): Embedding(2, 768)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder): BertEncoder(\n",
              "          (layer): ModuleList(\n",
              "            (0-11): 12 x BertLayer(\n",
              "              (attention): BertAttention(\n",
              "                (self): BertSdpaSelfAttention(\n",
              "                  (query): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (key): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (value): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (output): BertSelfOutput(\n",
              "                  (dense): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.1, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (intermediate): BertIntermediate(\n",
              "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "                (intermediate_act_fn): GELUActivation()\n",
              "              )\n",
              "              (output): BertOutput(\n",
              "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (pooler): BertPooler(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (activation): Tanh()\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the best model on the entire validation dataset\n",
        "model.eval()\n",
        "\n",
        "# Initialize counters for accuracy\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "# Loop over the validation set\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        total_correct += (predictions == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "# Calculate final accuracy\n",
        "accuracy = total_correct / total_samples * 100\n",
        "print(f\"Accuracy on the entire validation dataset: {accuracy:.2f}%\")\n",
        "\n",
        "# Optionally: Reverse logits and calculate accuracy again\n",
        "# def reverse_logits(logits):\n",
        "#     return logits * -1  # Reverse logits by multiplying by -1\n",
        "\n",
        "# # Test the model with reversed logits on the entire validation dataset\n",
        "# total_correct_reversed = 0\n",
        "# with torch.no_grad():\n",
        "#     for batch in val_dataloader:\n",
        "#         input_ids, attention_mask, labels = [item.to(device) for item in batch]\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits\n",
        "\n",
        "#         # Reverse the logits\n",
        "#         reversed_logits = reverse_logits(logits)\n",
        "#         reversed_predictions = torch.argmax(reversed_logits, dim=-1)\n",
        "\n",
        "#         # Calculate accuracy with reversed logits\n",
        "#         total_correct_reversed += (reversed_predictions == labels).sum().item()\n",
        "\n",
        "# # Calculate final accuracy with reversed logits\n",
        "# accuracy_reversed = total_correct_reversed / total_samples * 100\n",
        "# print(f\"Accuracy with reversed logits on the entire validation dataset: {accuracy_reversed:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo2Y8emlmiHq",
        "outputId": "ab108240-bbc6-4bf5-e905-e03166ac07c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the entire validation dataset: 58.50%\n"
          ]
        }
      ]
    }
  ]
}